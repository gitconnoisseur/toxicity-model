{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n"
     ]
    }
   ],
   "source": [
    "from tdc.single_pred import Tox\n",
    "import deepchem\n",
    "import xgboost\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.EState import Fingerprinter\n",
    "from rdkit.Chem.EState import EState\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import autokeras as ak\n",
    "\n",
    "from tdc.benchmark_group import admet_group\n",
    "group = admet_group(path = 'data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating training, validation splits...\n",
      "100%|██████████| 523/523 [00:00<00:00, 2050.90it/s]\n",
      "generating training, validation splits...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions for run #\n",
      "1\n",
      "[0.99999666, 0.9998036, 0.9999974, 0.9999937, 0.9999738]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 523/523 [00:00<00:00, 1248.10it/s]\n",
      "generating training, validation splits...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions for run #\n",
      "2\n",
      "[0.9999995, 0.9988067, 0.9999988, 0.9999994, 0.9999994]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 523/523 [00:00<00:00, 916.60it/s] \n",
      "generating training, validation splits...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions for run #\n",
      "3\n",
      "[1.0, 0.9999186, 0.9999999, 0.9999999, 0.9999995]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 523/523 [00:00<00:00, 1202.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 502 calls to <function KerasModel._create_gradient_fn.<locals>.apply_gradient_for_batch at 0x000001DBFEBC2CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating training, validation splits...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions for run #\n",
      "4\n",
      "[0.9999999, 0.9999722, 0.99999976, 0.9999999, 0.99999976]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 523/523 [00:00<00:00, 1161.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions for run #\n",
      "5\n",
      "[0.9999995, 0.9999267, 0.99999964, 0.9999994, 0.99999833]\n",
      "Prediction List:\n",
      "[{'herg': [0.99999666, 0.9998036, 0.9999974, 0.9999937, 0.9999738, 0.99714583, 0.9999745, 0.9999957, 0.99668735, 0.9997955, 0.99999034, 0.99991, 0.9999809, 0.9999721, 0.9999975, 0.99943084, 0.0012629897, 0.9907906, 0.06256882, 0.02853294, 0.08955188, 0.43349564, 0.9814311, 0.98701626, 0.9995994, 0.99295866, 0.2571235, 0.12053234, 0.56577384, 0.99635494, 0.8735955, 0.99992895, 0.9996817, 0.28208223, 0.99999654, 0.9999889, 0.9999536, 0.9955989, 0.0014255077, 0.8134548, 0.96316946, 0.0119785685, 0.99999785, 0.00046095485, 0.9972211, 0.9995912, 0.9999958, 0.9999958, 0.9913059, 0.9998704, 0.48803714, 0.7442882, 0.99977726, 0.83330965, 0.8962432, 0.9979875, 0.9353448, 0.9985312, 0.9711628, 0.0009999718, 0.025861848, 7.5785175e-07, 0.99986064, 0.9999409, 0.99901986, 0.999481, 0.99987245, 0.9817167, 0.99901986, 0.97940516, 0.99987245, 0.999481, 0.9070606, 0.9999386, 0.999946, 0.99983215, 0.999946, 0.9070606, 0.9817167, 0.99991035, 0.998543, 0.9999901, 0.99868757, 0.9998615, 0.999961, 0.999987, 0.9999658, 0.9999963, 0.9989287, 0.9999682, 0.09748429, 0.99998903, 0.99998903, 0.9970688, 0.02056009, 6.9733054e-05, 0.037944656, 0.9996867, 0.9996867, 0.99984074, 0.99984074, 0.02552162, 0.9999999, 0.9999273, 0.99972934, 0.9999374, 0.9999821, 0.9998944, 0.99998975, 0.99952996, 0.99958163, 0.9999281, 0.9998196, 0.9999732, 0.9970772, 0.99821633, 0.9988759, 0.98698366, 0.9991053, 0.9939778, 0.99266726, 0.999323, 0.9998585, 0.9941162, 0.9982765, 0.9901254, 0.9998599, 0.16734116, 0.9999299, 0.99776757, 0.99856335, 0.0024661534]}, {'herg': [0.9999995, 0.9988067, 0.9999988, 0.9999994, 0.9999994, 0.9993333, 0.999997, 0.9999995, 0.9997385, 0.98897994, 0.9999976, 0.9999571, 0.99888057, 0.9999633, 0.99943835, 0.99993634, 0.00011684334, 0.9965624, 0.023074923, 0.14890887, 0.29294595, 0.8430363, 0.9974285, 0.9954984, 0.9998839, 0.9967045, 0.567812, 0.300607, 0.9955362, 0.96459645, 0.9885318, 0.99998903, 0.9998079, 0.2856514, 0.99999845, 0.99999833, 0.9993394, 0.99496895, 0.00020994713, 0.97576296, 0.9858285, 0.16989654, 0.9999975, 5.169232e-05, 0.95088977, 0.98821837, 0.9999132, 0.99996567, 0.14423177, 0.98127913, 0.0042347913, 0.0008881264, 0.9992472, 0.8709351, 0.99997485, 0.9099902, 0.98068583, 0.9998522, 0.78181106, 0.002228218, 0.89457226, 4.725095e-06, 0.9996246, 0.9999739, 0.997532, 0.9999093, 0.99994826, 0.9979742, 0.997532, 0.98710686, 0.99994826, 0.9999093, 0.96648085, 0.99973506, 0.9999876, 0.9998735, 0.9999876, 0.96648085, 0.9979742, 0.9999845, 0.9991296, 0.9999976, 0.9997042, 0.9985454, 0.99949384, 0.9999993, 0.9999678, 0.9999982, 0.11298385, 0.9999652, 0.9380387, 0.99998367, 0.99998367, 0.9993049, 0.00093151536, 0.0010109957, 0.15342909, 0.9992939, 0.9992939, 0.99998057, 0.99998057, 0.84130704, 0.9996277, 0.99999547, 0.9999745, 0.99999607, 0.9999764, 0.9999974, 0.9999876, 0.9999927, 0.9999826, 0.9999989, 0.9999685, 0.9999896, 0.99312544, 0.9997651, 0.999954, 0.998689, 0.9999989, 0.9998977, 0.9791571, 0.9993291, 0.9999845, 0.99995184, 0.60926384, 0.96477294, 0.9999857, 0.06947708, 0.9999987, 0.9987257, 0.9995989, 0.058229655]}, {'herg': [1.0, 0.9999186, 0.9999999, 0.9999999, 0.9999995, 0.9988102, 0.9999999, 0.99999976, 0.9979226, 0.9998242, 0.9999819, 0.999997, 0.9999975, 0.99999905, 0.99999976, 0.9976439, 0.01589993, 0.99973065, 0.37894952, 0.027300151, 0.13922589, 0.40132752, 0.99604297, 0.9941907, 0.9999913, 0.99901986, 0.4686436, 0.31005174, 0.6783495, 0.9956584, 0.97710425, 0.9999269, 0.99986994, 0.055515457, 0.99996126, 0.9999343, 0.9998405, 0.99891794, 0.00050522754, 0.9801143, 0.9992536, 0.3864423, 0.9999989, 0.00044918974, 0.99985325, 0.9272009, 0.99997294, 0.99997926, 0.9600833, 0.99990106, 0.7468567, 0.43851823, 0.9991217, 0.9895449, 0.9999994, 0.99766266, 0.9849578, 0.99940884, 0.9182481, 0.0010042699, 0.95056343, 8.787229e-06, 0.99956125, 0.9999336, 0.9999217, 0.99993813, 0.99997723, 0.99979335, 0.9999217, 0.99894613, 0.99997723, 0.99993813, 0.97865945, 0.999833, 0.99881065, 0.99983597, 0.99881065, 0.97865945, 0.99979335, 0.9999757, 0.99993145, 0.99999905, 0.9998455, 0.99995947, 0.9996939, 0.9999919, 0.99997413, 0.9999999, 0.98382044, 0.9999486, 0.42269102, 0.9999999, 0.9999999, 0.999907, 0.008576748, 0.0016100738, 0.0012270614, 0.9995827, 0.9995827, 0.9998369, 0.9998369, 0.43624616, 0.9999223, 0.999987, 0.9999304, 0.99999094, 0.99999535, 0.99996305, 0.9999019, 0.9999423, 0.99989057, 0.9998722, 0.9999049, 0.99997723, 0.99920005, 0.9991229, 0.99980646, 0.9991756, 0.99996257, 0.99863917, 0.9870244, 0.9951349, 0.9999517, 0.9996748, 0.99401337, 0.89551777, 0.9998814, 0.0033515352, 0.99999464, 0.9968706, 0.99837273, 0.079532415]}, {'herg': [0.9999999, 0.9999722, 0.99999976, 0.9999999, 0.99999976, 0.9997975, 0.9999988, 0.9999999, 0.9802501, 0.9997483, 0.9999789, 0.9999982, 0.99999297, 0.99999774, 0.9999958, 0.9991953, 6.2729345e-07, 0.9998815, 0.14351982, 0.00384601, 0.9612009, 0.9872219, 0.9925478, 0.9846939, 0.99969363, 0.9992094, 0.6386655, 0.7180864, 0.660224, 0.8746693, 0.90568644, 0.9999893, 0.99997365, 0.024241008, 0.99999535, 0.99997795, 0.99996424, 0.9874347, 0.0005879424, 0.605936, 0.9694586, 0.05562595, 0.9999999, 0.00040167093, 0.99997604, 0.7141934, 0.99999607, 0.99999833, 0.96695054, 0.9979845, 0.98421556, 0.9836721, 0.99932015, 0.9566171, 0.9999236, 0.9985989, 0.9105983, 0.99981135, 0.91448826, 0.0006264834, 0.3987699, 7.3622286e-06, 0.99973947, 0.9997861, 0.9999244, 0.99999344, 0.999979, 0.9998118, 0.9999244, 0.9836264, 0.999979, 0.99999344, 0.9986099, 0.99999523, 0.9999956, 0.99999475, 0.9999956, 0.9986099, 0.9998118, 0.99995947, 0.99717796, 0.9999931, 0.99988544, 0.99363387, 0.9997347, 0.99999464, 0.9999443, 0.99999416, 0.9902774, 0.99992895, 0.84890014, 0.9999459, 0.9999459, 0.98424244, 0.0024475537, 0.0013976457, 0.0010094366, 0.99701893, 0.99701893, 0.9981329, 0.9981329, 0.31318715, 0.99997556, 0.99999523, 0.9999776, 0.9999969, 0.9999981, 0.9999343, 0.9999958, 0.9997918, 0.9998087, 0.99999774, 0.9999602, 0.99999976, 0.9989825, 0.9999554, 0.99998724, 0.99921346, 0.99999213, 0.9998301, 0.99335456, 0.99993443, 0.9999758, 0.99918693, 0.92609394, 0.8447484, 0.999798, 0.105636485, 0.99999094, 0.99988437, 0.99996424, 0.0018809902]}, {'herg': [0.9999995, 0.9999267, 0.99999964, 0.9999994, 0.99999833, 0.9838261, 0.9999974, 0.99999774, 0.99974674, 0.98643273, 0.9999839, 0.99996185, 0.999635, 0.99998283, 0.9999993, 0.9989637, 0.00015792612, 0.99761343, 0.40516946, 0.5707888, 0.021193115, 0.02984376, 0.8980979, 0.9998324, 0.99968505, 0.99653, 0.9195688, 0.90562004, 0.34350228, 0.982778, 0.2944068, 0.9999311, 0.9997563, 0.02179612, 0.999933, 0.9998597, 0.99923635, 0.99843913, 0.00011758505, 0.8159197, 0.99887675, 0.28444842, 0.9999995, 7.1204036e-06, 0.9991755, 0.51996166, 0.99998724, 0.99998784, 0.93734723, 0.9999622, 0.43839064, 0.37932593, 0.99889106, 0.9620072, 0.9999951, 0.988725, 0.9990245, 0.999003, 0.94700485, 2.622681e-05, 0.92799795, 2.8687986e-05, 0.9974215, 0.9997557, 0.9996489, 0.9999242, 0.99995065, 0.9994747, 0.9996489, 0.78219724, 0.99995065, 0.9999242, 0.86377954, 0.9996762, 0.99994695, 0.99956244, 0.99994695, 0.86377954, 0.9994747, 0.9997806, 0.9991493, 0.99999964, 0.9999832, 0.9999864, 0.9947785, 0.99998415, 0.9999534, 0.9999949, 0.99513584, 0.99999905, 0.42989781, 0.99999213, 0.99999213, 0.9951916, 0.021037897, 5.1818275e-05, 0.0032076524, 0.99999297, 0.99999297, 0.9999751, 0.9999751, 0.6234384, 0.9999999, 0.9999746, 0.99975103, 0.99996316, 0.9997875, 0.9999598, 0.99999344, 0.99968374, 0.99885833, 0.99998, 0.9998449, 0.999967, 0.9879479, 0.9902469, 0.99931085, 0.982746, 0.9997533, 0.99899083, 0.9264395, 0.9833438, 0.99880874, 0.9968939, 0.72740364, 0.99910104, 0.9983358, 0.011906363, 0.9987832, 0.99879324, 0.9911082, 0.029850991]}]\n"
     ]
    }
   ],
   "source": [
    "predictions_list = []\n",
    "metric = deepchem.deepchem.metrics.Metric(deepchem.deepchem.metrics.roc_auc_score)\n",
    "ecfpFeat = deepchem.deepchem.feat.CircularFingerprint(radius=4)\n",
    "convMolFeat = deepchem.deepchem.feat.ConvMolFeaturizer()\n",
    "xgb_reg_ecfp = xgboost.XGBClassifier()\n",
    "xgb_reg_estate = xgboost.XGBClassifier()\n",
    "\n",
    "for seed in [1, 2, 3, 4, 5]:\n",
    "    benchmark = group.get('herg') \n",
    "    \n",
    "    predictions = {}\n",
    "    name = benchmark['name']\n",
    "    train_val, test = benchmark['train_val'], benchmark['test']\n",
    "    train, valid = group.get_train_valid_split(benchmark = name, split_type = 'default', seed = seed)  \n",
    "\n",
    "    #trainMol = train.iloc[:,1].map(lambda x: Chem.MolFromSmiles(x))\n",
    "    #validMol = valid.iloc[:,1].map(lambda x: Chem.MolFromSmiles(x))\n",
    "    testMol = test.iloc[:,1].map(lambda x: Chem.MolFromSmiles(x))\n",
    "    train_valMol = train_val.iloc[:,1].map(lambda x: Chem.MolFromSmiles(x))\n",
    "\n",
    "    #featurize training, valid, and test data for xgboost\n",
    "    #trainFeat = np.stack(np.array(trainMol.map(lambda x: Fingerprinter.FingerprintMol(x)[1])))\n",
    "    #validFeat = np.stack(np.array(validMol.map(lambda x: Fingerprinter.FingerprintMol(x)[1])))\n",
    "    testFeat = np.stack(np.array(testMol.map(lambda x: Fingerprinter.FingerprintMol(x)[1])))\n",
    "    train_valFeat = np.stack(np.array(train_valMol.map(lambda x: Fingerprinter.FingerprintMol(x)[1])))\n",
    "\n",
    "    #featurize training, valid, and test data for xgboost\n",
    "    #ecfp_f_train = ecfpFeat.featurize(train.iloc[:,1].to_list())\n",
    "    #ecfp_f_valid = ecfpFeat.featurize(valid.iloc[:,1].to_list())\n",
    "    ecfp_f_test = ecfpFeat.featurize(test.iloc[:,1].to_list())\n",
    "    train_val_f = ecfpFeat.featurize(train_val.iloc[:,1].to_list())\n",
    "\n",
    "    #featurize training, valid, and test data for the GCN\n",
    "    cv_f_train = convMolFeat.featurize(train.iloc[:,1].to_list())\n",
    "    cv_f_valid = convMolFeat.featurize(valid.iloc[:,1].to_list())\n",
    "    cv_f_test = convMolFeat.featurize(test.iloc[:,1].to_list())\n",
    "\n",
    "    #convert training and validation data into a deepchem dataset for the gcn\n",
    "    gcn_train_data = deepchem.deepchem.data.NumpyDataset(X=cv_f_train, y=np.array(train.iloc[:,2]), ids=np.array(train.iloc[:,1].to_list()))\n",
    "    gcn_valid_data = deepchem.deepchem.data.NumpyDataset(X=cv_f_valid, y=np.array(valid.iloc[:,2]), ids=np.array(valid.iloc[:,1].to_list()))\n",
    "    gcn_test_data = deepchem.deepchem.data.NumpyDataset(X=cv_f_test, y=np.array(test.iloc[:,2]), ids=np.array(test.iloc[:,1].to_list()))\n",
    "\n",
    "    #fit data on GCN\n",
    "    reg = deepchem.deepchem.models.GraphConvModel(\n",
    "        n_tasks=1, \n",
    "        dropout=.001,\n",
    "        dense_layer_size=1577,\n",
    "        graph_conv_layers=[200, 200, 200],\n",
    "        mode=\"classification\",)\n",
    "    callback = deepchem.deepchem.models.ValidationCallback(gcn_valid_data, 1000, metric)\n",
    "    reg.fit(gcn_train_data, nb_epoch=100, callbacks=callback)\n",
    "\n",
    "    gcn_pred = reg.predict(dataset=gcn_test_data)\n",
    "    gcn_pred_processed = []\n",
    "    for prediction in gcn_pred:\n",
    "        gcn_pred_processed.append(prediction[0][1])\n",
    "\n",
    "    #fit xgboost model and store np ndarray in xgb_pred\n",
    "    xgb_reg_estate.fit(X=train_valFeat, y=train_val.iloc[:,2], eval_metric=\"auc\", verbose=True)\n",
    "    xgb_reg_ecfp.fit(X=train_val_f, y=train_val.iloc[:,2], eval_metric=\"auc\")\n",
    "    pred_estate = xgb_reg_estate.predict(X=testFeat)\n",
    "    pred_ecfp = xgb_reg_ecfp.predict(X=ecfp_f_test)\n",
    "\n",
    "    # store test predictions in y_pred_test\n",
    "    y_pred_test = gcn_pred_processed #np.round(np.mean([ pred_estate, pred_ecfp, gcn_pred_processed ], axis=0))\n",
    "\n",
    "    print(\"predictions for run #\")\n",
    "    print(seed)\n",
    "    print(y_pred_test[0:5])\n",
    "        \n",
    "    predictions[name] = y_pred_test\n",
    "    predictions_list.append(predictions)\n",
    "\n",
    "print(\"Prediction List:\")\n",
    "print(predictions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'herg': {'roc-auc': 0.775}}\n",
      "{'herg': {'roc-auc': 0.762}}\n",
      "{'herg': {'roc-auc': 0.734}}\n",
      "{'herg': {'roc-auc': 0.774}}\n",
      "{'herg': {'roc-auc': 0.746}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'herg': [0.758, 0.016]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in predictions_list:\n",
    "    print(group.evaluate(i))\n",
    "\n",
    "group.evaluate_many(predictions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "WARNING:tensorflow:5 out of the last 52 calls to <function KerasModel._create_gradient_fn.<locals>.apply_gradient_for_batch at 0x000001EF576FECA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "init   \t [7.19693503e-02 1.07500000e+03]. \t  0.7037037037037037 \t 0.7853751187084521\n",
      "init   \t [2.54595299e-02 9.08000000e+02]. \t  0.6467236467236468 \t 0.7853751187084521\n",
      "init   \t [1.48644776e-02 1.57800000e+03]. \t  0.7853751187084521 \t 0.7853751187084521\n",
      "1      \t [3.38025562e-02 1.08199998e+03]. \t  0.6999050332383665 \t 0.7853751187084521\n",
      "2      \t [4.16307587e-02 1.08107584e+03]. \t  0.741690408357075 \t 0.7853751187084521\n",
      "3      \t [1.00000000e-03 1.57719365e+03]. \t  \u001b[92m0.8499525166191833\u001b[0m \t 0.8499525166191833\n",
      "4      \t [2.50888308e-02 9.09034410e+02]. \t  0.7977207977207978 \t 0.8499525166191833\n",
      "5      \t [1.00000000e-01 1.57631682e+03]. \t  0.6514719848053181 \t 0.8499525166191833\n",
      "6      \t [1.0000000e-03 1.5790104e+03]. \t  0.698005698005698 \t 0.8499525166191833\n",
      "7      \t [1.00000000e-01 1.58010086e+03]. \t  0.717948717948718 \t 0.8499525166191833\n",
      "8      \t [3.001974e-02 1.094000e+03]. \t  0.758784425451092 \t 0.8499525166191833\n",
      "9      \t [8.12704802e-02 1.09312055e+03]. \t  0.7312440645773979 \t 0.8499525166191833\n",
      "10     \t [2.48520545e-02 9.09951185e+02]. \t  0.7037037037037037 \t 0.8499525166191833\n",
      "11     \t [1.62296569e-02 9.20000000e+02]. \t  0.7369420702754036 \t 0.8499525166191833\n",
      "12     \t [1.00000000e-01 1.09501318e+03]. \t  0.7141500474833808 \t 0.8499525166191833\n",
      "13     \t [9.02843873e-03 8.98000000e+02]. \t  0.7492877492877492 \t 0.8499525166191833\n",
      "14     \t [3.57934068e-03 8.98893487e+02]. \t  0.7958214624881292 \t 0.8499525166191833\n",
      "15     \t [1.00000000e-01 9.11067353e+02]. \t  0.7293447293447294 \t 0.8499525166191833\n",
      "16     \t [7.64945173e-02 1.07595726e+03]. \t  0.7321937321937322 \t 0.8499525166191833\n",
      "17     \t [1.00000000e-03 1.07701655e+03]. \t  0.7378917378917379 \t 0.8499525166191833\n",
      "18     \t [4.79294807e-02 9.19090585e+02]. \t  0.8233618233618234 \t 0.8499525166191833\n",
      "19     \t [1.00000000e-03 9.12070841e+02]. \t  0.7730294396961064 \t 0.8499525166191833\n",
      "20     \t [1.00000000e-03 1.58112686e+03]. \t  0.658119658119658 \t 0.8499525166191833\n"
     ]
    }
   ],
   "source": [
    "def model_builder(**model_params):\n",
    "  return deepchem.deepchem.models.GraphConvModel(n_tasks=1, mode=\"classification\", graph_conv_layers=[128, 128])\n",
    "\n",
    "optimizer = deepchem.deepchem.hyper.GaussianProcessHyperparamOpt(model_builder)\n",
    "\n",
    "params_dict = {\"dropout\": .01, \"dense_layer_size\": 200}\n",
    "\n",
    "best_model, best_hyperparams, all_results = optimizer.hyperparam_search(params_dict, gcn_train_data, gcn_valid_data, metric, search_range=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dropout': 0.001, 'dense_layer_size': 1577}\n",
      "{'_dense_layer_size_1075_dropout_0.071969': 0.7037037037037037, '_dense_layer_size_908_dropout_0.025460': 0.6467236467236468, '_dense_layer_size_1578_dropout_0.014864': 0.7853751187084521, '_dense_layer_size_1081_dropout_0.033803': 0.6999050332383665, '_dense_layer_size_1081_dropout_0.041631': 0.741690408357075, '_dense_layer_size_1577_dropout_0.001000': 0.8499525166191833, '_dense_layer_size_909_dropout_0.025089': 0.7977207977207978, '_dense_layer_size_1576_dropout_0.100000': 0.6514719848053181, '_dense_layer_size_1579_dropout_0.001000': 0.698005698005698, '_dense_layer_size_1580_dropout_0.100000': 0.717948717948718, '_dense_layer_size_1094_dropout_0.030020': 0.758784425451092, '_dense_layer_size_1093_dropout_0.081270': 0.7312440645773979, '_dense_layer_size_909_dropout_0.024852': 0.7037037037037037, '_dense_layer_size_920_dropout_0.016230': 0.7369420702754036, '_dense_layer_size_1095_dropout_0.100000': 0.7141500474833808, '_dense_layer_size_898_dropout_0.009028': 0.7492877492877492, '_dense_layer_size_898_dropout_0.003579': 0.7958214624881292, '_dense_layer_size_911_dropout_0.100000': 0.7293447293447294, '_dense_layer_size_1075_dropout_0.076495': 0.7321937321937322, '_dense_layer_size_1077_dropout_0.001000': 0.7378917378917379, '_dense_layer_size_919_dropout_0.047929': 0.8233618233618234, '_dense_layer_size_912_dropout_0.001000': 0.7730294396961064, '_dense_layer_size_1581_dropout_0.001000': 0.658119658119658}\n"
     ]
    }
   ],
   "source": [
    "print(best_hyperparams)\n",
    "print(all_results)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2d9ef161e060c37aa994177a4d833cab797e22621213b4f88b9fe284585c74c2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('deepchem-test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
